{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ddkaba/IAD_Lab_3/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAKS9BPmAMlE"
      },
      "source": [
        "ОЛАПЛОРАДЛОРОЛАР"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорты\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Скачивание и распаковка архива\n",
        "url = 'https://www.kaggle.com/api/v1/datasets/download/alxmamaev/flowers-recognition'\n",
        "zip_path = 'flowers-recognition.zip'\n",
        "extract_path = '.'\n",
        "\n",
        "# Скачивание\n",
        "print('Скачивание архива...')\n",
        "response = requests.get(url, allow_redirects=True)\n",
        "with open(zip_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "print(f'Архив скачан: {zip_path}')\n",
        "\n",
        "# Распаковка\n",
        "print('Распаковка архива...')\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print('Архив распакован')\n",
        "\n",
        "# Путь к данным и показать классы\n",
        "data_path = 'flowers/flowers'\n",
        "print(f'\\nПуть к данным: {data_path}')\n",
        "print('Классы цветов:')\n",
        "for flower_class in os.listdir(data_path):\n",
        "    class_path = os.path.join(data_path, flower_class)\n",
        "    if os.path.isdir(class_path):\n",
        "        count = len(os.listdir(class_path))\n",
        "        print(f'  - {flower_class}: {count} изображений')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Параметры для преобразования\n",
        "target_size = (150, 150)  # Целевой размер всех изображений\n",
        "\n",
        "print(f'Целевой размер изображений: {target_size[0]}x{target_size[1]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для преобразования изображения\n",
        "def preprocess_image(img_path, target_size):\n",
        "    \"\"\"\n",
        "    1. Читает JPEG файл\n",
        "    2. Декодирует в RGB\n",
        "    3. Изменяет размер до target_size\n",
        "    4. Возвращает PIL изображение\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path)\n",
        "    # Конвертация в RGB (на случай RGBA или grayscale)\n",
        "    img = img.convert('RGB')\n",
        "    # Изменение размера\n",
        "    img = img.resize(target_size, Image.LANCZOS)\n",
        "    return img\n",
        "\n",
        "print('Функция преобразования создана')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Преобразование всех изображений в исходной папке\n",
        "original_dataset_dir = 'flowers/flowers'\n",
        "\n",
        "print('Начало преобразования изображений...\\n')\n",
        "\n",
        "classes = [d for d in os.listdir(original_dataset_dir) \n",
        "           if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
        "\n",
        "total_processed = 0\n",
        "for flower_class in classes:\n",
        "    class_dir = os.path.join(original_dataset_dir, flower_class)\n",
        "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    \n",
        "    print(f'Обработка класса \"{flower_class}\": {len(images)} изображений...')\n",
        "    \n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(class_dir, img_name)\n",
        "        \n",
        "        try:\n",
        "            # Преобразование\n",
        "            img = preprocess_image(img_path, target_size)\n",
        "            # Сохранение обратно\n",
        "            img.save(img_path, 'JPEG', quality=95)\n",
        "            total_processed += 1\n",
        "        except Exception as e:\n",
        "            print(f'  Ошибка при обработке {img_name}: {e}')\n",
        "    \n",
        "    print(f'  ✓ Обработано: {len(images)} изображений')\n",
        "\n",
        "print(f'\\nВсего преобразовано: {total_processed} изображений')\n",
        "print(f'Все изображения теперь имеют размер {target_size[0]}x{target_size[1]} пикселей')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверка: показать несколько преобразованных изображений\n",
        "print('Примеры преобразованных изображений:\\n')\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "idx = 0\n",
        "for flower_class in classes[:2]:  # Первые 2 класса\n",
        "    class_dir = os.path.join(original_dataset_dir, flower_class)\n",
        "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:4]\n",
        "    \n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(class_dir, img_name)\n",
        "        img = Image.open(img_path)\n",
        "        \n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f'{flower_class}\\n{img.size[0]}x{img.size[1]} px')\n",
        "        axes[idx].axis('off')\n",
        "        idx += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание структуры папок для train/validation/test\n",
        "original_dataset_dir = 'flowers/flowers'\n",
        "base_dir = 'flowers_dataset'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Создание основных директорий\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Получить список всех классов\n",
        "classes = [d for d in os.listdir(original_dataset_dir) \n",
        "           if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
        "\n",
        "print(f'Найдено классов: {len(classes)}')\n",
        "print(f'Классы: {classes}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание поддиректорий для каждого класса\n",
        "for flower_class in classes:\n",
        "    os.makedirs(os.path.join(train_dir, flower_class), exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_dir, flower_class), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, flower_class), exist_ok=True)\n",
        "\n",
        "print('Структура папок создана')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Копирование изображений в соответствующие папки\n",
        "# Разделение: первые 60% - train, следующие 20% - validation, последние 20% - test\n",
        "\n",
        "for flower_class in classes:\n",
        "    src_dir = os.path.join(original_dataset_dir, flower_class)\n",
        "    fnames = os.listdir(src_dir)\n",
        "    \n",
        "    total = len(fnames)\n",
        "    train_count = int(0.6 * total)\n",
        "    val_count = int(0.2 * total)\n",
        "    \n",
        "    # Train\n",
        "    train_fnames = fnames[:train_count]\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(src_dir, fname)\n",
        "        dst = os.path.join(train_dir, flower_class, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "    # Validation\n",
        "    val_fnames = fnames[train_count:train_count + val_count]\n",
        "    for fname in val_fnames:\n",
        "        src = os.path.join(src_dir, fname)\n",
        "        dst = os.path.join(validation_dir, flower_class, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "    # Test\n",
        "    test_fnames = fnames[train_count + val_count:]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(src_dir, fname)\n",
        "        dst = os.path.join(test_dir, flower_class, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "    print(f'{flower_class}: train={len(train_fnames)}, val={len(val_fnames)}, test={len(test_fnames)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вывод структуры папок с размерами\n",
        "def get_dir_size(path):\n",
        "    \"\"\"Вычисляет размер папки в МБ\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.exists(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # Конвертация в МБ\n",
        "\n",
        "print('Структура папок с размерами:\\n')\n",
        "print(f'{base_dir}/ - {get_dir_size(base_dir):.2f} МБ')\n",
        "\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    split_size = get_dir_size(split_dir)\n",
        "    print(f'  {split}/ - {split_size:.2f} МБ')\n",
        "    \n",
        "    for flower_class in sorted(classes):\n",
        "        class_dir = os.path.join(split_dir, flower_class)\n",
        "        class_size = get_dir_size(class_dir)\n",
        "        count = len(os.listdir(class_dir))\n",
        "        print(f'    {flower_class}/ - {class_size:.2f} МБ ({count} изображений)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Параметры для создания тензоров\n",
        "img_height = 150\n",
        "img_width = 150\n",
        "batch_size = 32\n",
        "\n",
        "print(f'Параметры преобразования в тензоры:')\n",
        "print(f'  Размер: {img_height}x{img_width}')\n",
        "print(f'  Размер батча: {batch_size}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание датасетов с использованием image_dataset_from_directory\n",
        "# Эта функция автоматически:\n",
        "# 1) Составляет список подкаталогов (классов)\n",
        "# 2) Индексирует файлы изображений в каждом подкаталоге\n",
        "# 3) Создает tf.data.Dataset\n",
        "# 4) Читает файлы JPEG\n",
        "# 5) Декодирует в RGB\n",
        "# 6) Преобразует в тензоры\n",
        "# 7) Приводит к общему размеру\n",
        "# 8) Упаковывает в батчи\n",
        "\n",
        "# Обучающая выборка\n",
        "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Валидационная выборка\n",
        "validation_dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Тестовая выборка\n",
        "test_dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    shuffle=False  # Не перемешиваем тест для корректной оценки\n",
        ")\n",
        "\n",
        "print('\\n✓ Датасеты успешно созданы с помощью image_dataset_from_directory')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Нормализация значений пикселей [0, 255] -> [0, 1]\n",
        "# Создаем слой нормализации\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "# Применяем нормализацию к датасетам\n",
        "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "print('Нормализация применена: [0, 255] -> [0, 1]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Аугментация данных для обучающей выборки\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomTranslation(0.1, 0.1),\n",
        "])\n",
        "\n",
        "# Применяем аугментацию только к обучающей выборке\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ")\n",
        "\n",
        "print('Аугментация данных добавлена к обучающей выборке')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оптимизация производительности\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print('Оптимизация датасетов завершена (prefetch)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверка: получение и визуализация батча тензоров\n",
        "sample_batch, sample_labels = next(iter(train_dataset))\n",
        "\n",
        "print('Проверка преобразования в тензоры:\\n')\n",
        "print(f'Форма батча изображений: {sample_batch.shape}')\n",
        "print(f'  - Батч содержит {sample_batch.shape[0]} изображений')\n",
        "print(f'  - Каждое изображение: {sample_batch.shape[1]}x{sample_batch.shape[2]} пикселей')\n",
        "print(f'  - Количество каналов (RGB): {sample_batch.shape[3]}')\n",
        "print(f'\\nФорма батча меток: {sample_labels.shape}')\n",
        "print(f'  - One-hot encoding для {sample_labels.shape[1]} классов')\n",
        "print(f'\\nДиапазон значений пикселей в тензоре: [{sample_batch.numpy().min():.3f}, {sample_batch.numpy().max():.3f}]')\n",
        "print(f'Тип данных: {sample_batch.dtype}')\n",
        "\n",
        "# Получение имен классов\n",
        "class_names = train_dataset.class_names if hasattr(train_dataset, 'class_names') else sorted(os.listdir(train_dir))\n",
        "print(f'\\nКлассы: {class_names}')\n",
        "print(f'\\n✓ Изображения успешно преобразованы в тензоры с вещественными числами [0, 1]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация батча преобразованных изображений\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Получаем классы из каталога\n",
        "class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
        "\n",
        "for i in range(12):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    # Клиппинг значений на случай если аугментация дала значения вне [0, 1]\n",
        "    img_to_show = np.clip(sample_batch[i].numpy(), 0, 1)\n",
        "    plt.imshow(img_to_show)\n",
        "    \n",
        "    # Получить название класса\n",
        "    class_idx = np.argmax(sample_labels[i])\n",
        "    class_name = class_names[class_idx]\n",
        "    \n",
        "    plt.title(f'{class_name}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Примеры изображений из батча (тензоры нормализованы [0, 1])', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nИзображения готовы к подаче в нейронную сеть!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ЗАДАНИЕ 1: Построение сверточной нейронной сети\n",
        "# Архитектура: Conv2D + MaxPooling + Dense классификатор\n",
        "\n",
        "num_classes = 5  # Количество классов цветов\n",
        "l2_reg = 0.001  # Коэффициент L2 регуляризации\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Входной слой\n",
        "    layers.Input(shape=(150, 150, 3)),\n",
        "    \n",
        "    # Блок 1: Conv2D + MaxPooling\n",
        "    layers.Conv2D(32, kernel_size=3, activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    # Блок 2: Conv2D + MaxPooling\n",
        "    layers.Conv2D(64, kernel_size=3, activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    # Блок 3: Conv2D + MaxPooling\n",
        "    layers.Conv2D(128, kernel_size=3, activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    # Блок 4: Conv2D + MaxPooling\n",
        "    layers.Conv2D(256, kernel_size=3, activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    # Блок 5: Conv2D + MaxPooling\n",
        "    layers.Conv2D(256, kernel_size=3, activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    # Flatten для преобразования карт признаков в вектор\n",
        "    layers.Flatten(),\n",
        "    \n",
        "    # Полносвязный классификатор (не менее 2 слоев)\n",
        "    # Комбинируем Dropout + L2 регуляризацию для борьбы с переобучением\n",
        "    layers.Dense(512, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(256, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    # Выходной слой для 5 классов\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Просмотр архитектуры\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Компиляция модели\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # Для многоклассовой классификации\n",
        "    optimizer='adam',  # Adam оптимизатор (можно заменить на 'rmsprop')\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print('✓ Модель скомпилирована')\n",
        "print(f'Оптимизатор: Adam')\n",
        "print(f'Функция потерь: categorical_crossentropy')\n",
        "print(f'Метрики: accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение модели с валидацией\n",
        "epochs = 30\n",
        "\n",
        "print(f'Начало обучения на {epochs} эпох...\\n')\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('\\n✓ Обучение завершено')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Построение графиков точности и потерь\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# График точности\n",
        "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Точность модели (Accuracy)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[0].set_ylabel('Точность', fontsize=12)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# График потерь\n",
        "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Потери модели (Loss)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[1].set_ylabel('Потери', fontsize=12)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Вывод финальных метрик\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f'\\nФинальные метрики:')\n",
        "print(f'Train Accuracy: {final_train_acc:.4f}')\n",
        "print(f'Validation Accuracy: {final_val_acc:.4f}')\n",
        "print(f'Train Loss: {final_train_loss:.4f}')\n",
        "print(f'Validation Loss: {final_val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оценка на тестовой выборке\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "print(f'\\n{\"=\"*50}')\n",
        "print(f'РЕЗУЛЬТАТЫ НА ТЕСТОВОЙ ВЫБОРКЕ:')\n",
        "print(f'{\"=\"*50}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'{\"=\"*50}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохранение модели (в современном формате Keras)\n",
        "model.save('model_cnn_custom.keras')\n",
        "print('✓ Модель сохранена: model_cnn_custom.keras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ЗАДАНИЕ 2: Трансферное обучение с предобученной моделью EfficientNetB1\n",
        "\n",
        "**Подход:** Использование сверточного блока предобученной модели EfficientNetB1 (обученной на ImageNet) с добавлением нового классификатора для 5 классов цветов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка предобученной модели EfficientNetB1\n",
        "from tensorflow.keras.applications import EfficientNetB1\n",
        "\n",
        "# Создание базовой модели (сверточная основа)\n",
        "base_model = EfficientNetB1(\n",
        "    weights='imagenet',        # Веса, обученные на ImageNet\n",
        "    include_top=False,         # Без полносвязного классификатора (будем добавлять свой)\n",
        "    input_shape=(150, 150, 3)  # Размер входных изображений\n",
        ")\n",
        "\n",
        "# Замораживаем веса сверточной основы\n",
        "# (не будем обучать предобученные слои на первом этапе)\n",
        "base_model.trainable = False\n",
        "\n",
        "print(f'Предобученная модель: EfficientNetB1')\n",
        "print(f'Количество слоев в базовой модели: {len(base_model.layers)}')\n",
        "print(f'Веса заморожены: {not base_model.trainable}')\n",
        "print(f'\\nАрхитектура базовой модели:')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Построение полной модели с новым классификатором\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "# Создаем новую модель\n",
        "inputs = Input(shape=(150, 150, 3))\n",
        "\n",
        "# Нормализация для EfficientNet (встроенная предобработка)\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "# Пропускаем через предобученную сверточную основу\n",
        "x = base_model(x, training=False)  # training=False важно для BatchNormalization\n",
        "\n",
        "# Добавляем новый классификатор\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Преобразуем карты признаков в вектор\n",
        "x = layers.Dropout(0.2)(x)              # Легкая регуляризация\n",
        "\n",
        "# Полносвязные слои\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Выходной слой\n",
        "outputs = layers.Dense(5, activation='softmax')(x)\n",
        "\n",
        "# Создаем модель\n",
        "model_transfer = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print('✓ Модель с трансферным обучением создана')\n",
        "print(f'\\nАрхитектура:')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Просмотр архитектуры модели\n",
        "model_transfer.summary()\n",
        "\n",
        "# Подсчет параметров\n",
        "trainable_params = sum([np.prod(v.shape) for v in model_transfer.trainable_weights])\n",
        "non_trainable_params = sum([np.prod(v.shape) for v in model_transfer.non_trainable_weights])\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'Обучаемые параметры: {trainable_params:,}')\n",
        "print(f'Замороженные параметры: {non_trainable_params:,}')\n",
        "print(f'Всего параметров: {trainable_params + non_trainable_params:,}')\n",
        "print(f'{\"=\"*60}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Компиляция модели\n",
        "model_transfer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print('✓ Модель скомпилирована')\n",
        "print('Оптимизатор: Adam')\n",
        "print('Функция потерь: categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ЭТАП 1: Обучение только нового классификатора (база заморожена)\n",
        "epochs_stage1 = 20\n",
        "\n",
        "print(f'ЭТАП 1: Обучение классификатора ({epochs_stage1} эпох)')\n",
        "print('Сверточная основа EfficientNetB1 заморожена\\n')\n",
        "\n",
        "history_stage1 = model_transfer.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs_stage1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('\\n✓ Этап 1 завершен')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Графики для Этапа 1\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# График точности\n",
        "axes[0].plot(history_stage1.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history_stage1.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Этап 1: Точность (только классификатор)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[0].set_ylabel('Точность', fontsize=12)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# График потерь\n",
        "axes[1].plot(history_stage1.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history_stage1.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Этап 1: Потери (только классификатор)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[1].set_ylabel('Потери', fontsize=12)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Финальные метрики Этапа 1\n",
        "final_train_acc_s1 = history_stage1.history['accuracy'][-1]\n",
        "final_val_acc_s1 = history_stage1.history['val_accuracy'][-1]\n",
        "\n",
        "print(f'\\nФинальные метрики Этапа 1:')\n",
        "print(f'Train Accuracy: {final_train_acc_s1:.4f}')\n",
        "print(f'Validation Accuracy: {final_val_acc_s1:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ЭТАП 2: Fine-tuning - разморозка части сверточной основы\n",
        "# Размораживаем последние слои базовой модели для тонкой настройки\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "# Замораживаем первые 80% слоев, размораживаем последние 20%\n",
        "fine_tune_at = int(len(base_model.layers) * 0.8)\n",
        "\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers[fine_tune_at:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "print(f'ЭТАП 2: Fine-tuning')\n",
        "print(f'Всего слоев в базовой модели: {len(base_model.layers)}')\n",
        "print(f'Заморожено слоев: {fine_tune_at}')\n",
        "print(f'Обучаемых слоев: {len(base_model.layers) - fine_tune_at}')\n",
        "\n",
        "# Пересчитываем параметры\n",
        "trainable_params = sum([np.prod(v.shape) for v in model_transfer.trainable_weights])\n",
        "non_trainable_params = sum([np.prod(v.shape) for v in model_transfer.non_trainable_weights])\n",
        "\n",
        "print(f'\\nОбучаемые параметры: {trainable_params:,}')\n",
        "print(f'Замороженные параметры: {non_trainable_params:,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Перекомпиляция с меньшим learning rate для fine-tuning\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model_transfer.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # Очень маленький learning rate\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print('✓ Модель перекомпилирована с learning_rate=1e-5 для fine-tuning')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение Этапа 2\n",
        "epochs_stage2 = 15\n",
        "\n",
        "print(f'Обучение Этапа 2: fine-tuning ({epochs_stage2} эпох)\\n')\n",
        "\n",
        "history_stage2 = model_transfer.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs_stage2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('\\n✓ Этап 2 (fine-tuning) завершен')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Графики для Этапа 2\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# График точности\n",
        "axes[0].plot(history_stage2.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history_stage2.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Этап 2: Точность (fine-tuning)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[0].set_ylabel('Точность', fontsize=12)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# График потерь\n",
        "axes[1].plot(history_stage2.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history_stage2.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Этап 2: Потери (fine-tuning)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[1].set_ylabel('Потери', fontsize=12)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Финальные метрики Этапа 2\n",
        "final_train_acc_s2 = history_stage2.history['accuracy'][-1]\n",
        "final_val_acc_s2 = history_stage2.history['val_accuracy'][-1]\n",
        "\n",
        "print(f'\\nФинальные метрики Этапа 2:')\n",
        "print(f'Train Accuracy: {final_train_acc_s2:.4f}')\n",
        "print(f'Validation Accuracy: {final_val_acc_s2:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Объединенные графики обоих этапов\n",
        "# Concatenate history\n",
        "total_epochs_s1 = len(history_stage1.history['accuracy'])\n",
        "total_epochs_s2 = len(history_stage2.history['accuracy'])\n",
        "\n",
        "acc = history_stage1.history['accuracy'] + history_stage2.history['accuracy']\n",
        "val_acc = history_stage1.history['val_accuracy'] + history_stage2.history['val_accuracy']\n",
        "loss = history_stage1.history['loss'] + history_stage2.history['loss']\n",
        "val_loss = history_stage1.history['val_loss'] + history_stage2.history['val_loss']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# График точности\n",
        "axes[0].plot(acc, label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(val_acc, label='Validation Accuracy', linewidth=2)\n",
        "axes[0].axvline(x=total_epochs_s1-1, color='red', linestyle='--', linewidth=1.5, \n",
        "                label='Начало Fine-tuning')\n",
        "axes[0].set_title('Полная история: Точность (Этап 1 + Этап 2)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[0].set_ylabel('Точность', fontsize=12)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# График потерь\n",
        "axes[1].plot(loss, label='Train Loss', linewidth=2)\n",
        "axes[1].plot(val_loss, label='Validation Loss', linewidth=2)\n",
        "axes[1].axvline(x=total_epochs_s1-1, color='red', linestyle='--', linewidth=1.5,\n",
        "                label='Начало Fine-tuning')\n",
        "axes[1].set_title('Полная история: Потери (Этап 1 + Этап 2)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Эпоха', fontsize=12)\n",
        "axes[1].set_ylabel('Потери', fontsize=12)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оценка на тестовой выборке\n",
        "test_loss_transfer, test_accuracy_transfer = model_transfer.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "print(f'\\n{\"=\"*50}')\n",
        "print(f'РЕЗУЛЬТАТЫ НА ТЕСТОВОЙ ВЫБОРКЕ (Transfer Learning):')\n",
        "print(f'{\"=\"*50}')\n",
        "print(f'Test Accuracy: {test_accuracy_transfer:.4f} ({test_accuracy_transfer*100:.2f}%)')\n",
        "print(f'Test Loss: {test_loss_transfer:.4f}')\n",
        "print(f'{\"=\"*50}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохранение модели с трансферным обучением\n",
        "model_transfer.save('model_efficientnetb1_transfer.keras')\n",
        "print('✓ Модель сохранена: model_efficientnetb1_transfer.keras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ЗАДАНИЕ 2 (Альтернативный подход): Извлечение признаков из предобученной модели\n",
        "\n",
        "**Подход из лекций:** Извлечь признаки через сверточную основу EfficientNetB1 один раз, сохранить в NumPy массивы, затем обучить простой Dense классификатор на готовых признаках.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание сверточной основы для извлечения признаков\n",
        "conv_base = EfficientNetB1(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(150, 150, 3)\n",
        ")\n",
        "\n",
        "print('✓ Сверточная основа EfficientNetB1 загружена')\n",
        "print(f'Форма выхода conv_base: {conv_base.output_shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для извлечения признаков из датасета\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "def extract_features(dataset, sample_count):\n",
        "    \"\"\"\n",
        "    Извлекает признаки из датасета с помощью conv_base\n",
        "    \"\"\"\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    samples_processed = 0\n",
        "    \n",
        "    for images_batch, labels_batch in dataset:\n",
        "        # Предобработка для EfficientNet\n",
        "        preprocessed = preprocess_input(images_batch.numpy() * 255.0)\n",
        "        \n",
        "        # Извлечение признаков\n",
        "        features_batch = conv_base.predict(preprocessed, verbose=0)\n",
        "        \n",
        "        features_list.append(features_batch)\n",
        "        labels_list.append(labels_batch.numpy())\n",
        "        \n",
        "        samples_processed += images_batch.shape[0]\n",
        "        if samples_processed >= sample_count:\n",
        "            break\n",
        "    \n",
        "    features = np.concatenate(features_list, axis=0)[:sample_count]\n",
        "    labels = np.concatenate(labels_list, axis=0)[:sample_count]\n",
        "    \n",
        "    return features, labels\n",
        "\n",
        "print('✓ Функция извлечения признаков создана')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Извлечение признаков для всех выборок\n",
        "print('Извлечение признаков из датасетов...\\n')\n",
        "\n",
        "# Подсчет количества образцов\n",
        "train_samples = sum([1 for _ in train_dataset.unbatch()])\n",
        "val_samples = sum([1 for _ in validation_dataset.unbatch()])\n",
        "test_samples = sum([1 for _ in test_dataset.unbatch()])\n",
        "\n",
        "print(f'Train: {train_samples} образцов')\n",
        "print(f'Validation: {val_samples} образцов')\n",
        "print(f'Test: {test_samples} образцов\\n')\n",
        "\n",
        "print('Извлечение признаков из train...')\n",
        "train_features, train_labels = extract_features(train_dataset, train_samples)\n",
        "print(f'✓ Train features: {train_features.shape}')\n",
        "\n",
        "print('Извлечение признаков из validation...')\n",
        "val_features, val_labels = extract_features(validation_dataset, val_samples)\n",
        "print(f'✓ Validation features: {val_features.shape}')\n",
        "\n",
        "print('Извлечение признаков из test...')\n",
        "test_features, test_labels = extract_features(test_dataset, test_samples)\n",
        "print(f'✓ Test features: {test_features.shape}')\n",
        "\n",
        "print('\\n✓ Все признаки извлечены и сохранены в NumPy массивы')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten признаков для полносвязной сети\n",
        "train_features_flat = train_features.reshape((train_features.shape[0], -1))\n",
        "val_features_flat = val_features.reshape((val_features.shape[0], -1))\n",
        "test_features_flat = test_features.reshape((test_features.shape[0], -1))\n",
        "\n",
        "print(f'Форма после flatten:')\n",
        "print(f'Train: {train_features_flat.shape}')\n",
        "print(f'Validation: {val_features_flat.shape}')\n",
        "print(f'Test: {test_features_flat.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Построение полносвязного классификатора для готовых признаков\n",
        "feature_input_shape = train_features_flat.shape[1]\n",
        "\n",
        "model_features = models.Sequential([\n",
        "    layers.Input(shape=(feature_input_shape,)),\n",
        "    \n",
        "    # Полносвязный классификатор с регуляризацией\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    # Выходной слой\n",
        "    layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model_features.summary()\n",
        "\n",
        "print(f'\\n✓ Полносвязный классификатор построен')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Компиляция модели\n",
        "model_features.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print('✓ Модель скомпилирована')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение на извлеченных признаках с callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "epochs_features = 80\n",
        "\n",
        "# Настройка callbacks (как в лекциях)\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_efficientnetb1_features.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        verbose=1,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=7,\n",
        "        verbose=1,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f'Обучение классификатора на готовых признаках ({epochs_features} эпох)')\n",
        "print('Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\\n')\n",
        "\n",
        "history_features = model_features.fit(\n",
        "    train_features_flat, train_labels,\n",
        "    validation_data=(val_features_flat, val_labels),\n",
        "    epochs=epochs_features,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('\\n✓ Обучение завершено')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Построение графиков (в стиле лекций)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "epochs_range = range(1, len(history_features.history['accuracy']) + 1)\n",
        "\n",
        "# График точности\n",
        "axes[0].plot(epochs_range, history_features.history['accuracy'], 'o', \n",
        "             label='Точность на этапе обучения', markersize=4)\n",
        "axes[0].plot(epochs_range, history_features.history['val_accuracy'], '-', \n",
        "             label='Точность на этапе проверки', linewidth=2)\n",
        "axes[0].set_title('Точность на этапах обучения и проверки', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xlabel('Эпоха', fontsize=11)\n",
        "axes[0].set_ylabel('Точность', fontsize=11)\n",
        "axes[0].set_ylim([0.9, 1.0])\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# График потерь\n",
        "axes[1].plot(epochs_range, history_features.history['loss'], 'o',\n",
        "             label='Потери на этапе обучения', markersize=4)\n",
        "axes[1].plot(epochs_range, history_features.history['val_loss'], '-',\n",
        "             label='Потери на этапе проверки', linewidth=2)\n",
        "axes[1].set_title('Потери на этапах обучения и проверки', fontsize=13, fontweight='bold')\n",
        "axes[1].set_xlabel('Эпоха', fontsize=11)\n",
        "axes[1].set_ylabel('Потери', fontsize=11)\n",
        "axes[1].set_ylim([0, 12])\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Финальные метрики\n",
        "final_train_acc = history_features.history['accuracy'][-1]\n",
        "final_val_acc = history_features.history['val_accuracy'][-1]\n",
        "final_train_loss = history_features.history['loss'][-1]\n",
        "final_val_loss = history_features.history['val_loss'][-1]\n",
        "\n",
        "print(f'\\nФинальные метрики:')\n",
        "print(f'Train Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)')\n",
        "print(f'Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)')\n",
        "print(f'Train Loss: {final_train_loss:.4f}')\n",
        "print(f'Validation Loss: {final_val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оценка на тестовой выборке\n",
        "test_loss_features, test_accuracy_features = model_features.evaluate(\n",
        "    test_features_flat, test_labels, verbose=0\n",
        ")\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'РЕЗУЛЬТАТЫ НА ТЕСТОВОЙ ВЫБОРКЕ (Feature Extraction):')\n",
        "print(f'{\"=\"*60}')\n",
        "print(f'Test Accuracy: {test_accuracy_features:.4f} ({test_accuracy_features*100:.2f}%)')\n",
        "print(f'Test Loss: {test_loss_features:.4f}')\n",
        "print(f'{\"=\"*60}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохранение модели\n",
        "model_features.save('model_efficientnetb1_features.keras')\n",
        "print('✓ Модель сохранена: model_efficientnetb1_features.keras')\n",
        "\n",
        "# Опционально: сохранение признаков на диск\n",
        "np.save('train_features.npy', train_features_flat)\n",
        "np.save('val_features.npy', val_features_flat)\n",
        "np.save('test_features.npy', test_features_flat)\n",
        "np.save('train_labels.npy', train_labels)\n",
        "np.save('val_labels.npy', val_labels)\n",
        "np.save('test_labels.npy', test_labels)\n",
        "\n",
        "print('✓ Признаки сохранены в .npy файлы')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMnzDxjyL48jISU/nvp2GSO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
